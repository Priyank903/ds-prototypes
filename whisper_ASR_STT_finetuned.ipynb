{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Load the newly uploaded CSV file\n",
    "csv_file_path = '/home/priyank/Dataset_Folder/csv/dataset_hindi_only_updated.csv'\n",
    "df_audio = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Display the first few rows of the CSV to check its contents\n",
    "df_audio.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict, Audio\n",
    "import numpy as np\n",
    "\n",
    "def create_dataset_from_csv(csv_path, train_test_split=0.99, seed=42):\n",
    "    \"\"\"\n",
    "    Create a DatasetDict from a CSV file, keeping only audio and sentence columns.\n",
    "    \n",
    "    Args:\n",
    "        csv_path (str): Path to the CSV file\n",
    "        train_test_split (float): Proportion of data to use for training\n",
    "        seed (int): Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "        DatasetDict: Dataset with only audio and sentence features\n",
    "    \"\"\"\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Create a copy of audio_path column\n",
    "    df['audio'] = df['local_audio_path'].apply(lambda x: {'path': x})\n",
    "    \n",
    "    # Rename transcript to sentence\n",
    "    df = df.rename(columns={'transcripts-flash-2.0-001': 'sentence'})\n",
    "    \n",
    "    # Keep only the required columns\n",
    "    df = df[['audio', 'sentence']]\n",
    "    \n",
    "    # Create Dataset object\n",
    "    dataset = Dataset.from_pandas(df)\n",
    "    \n",
    "    # Cast the audio column to Audio feature\n",
    "    dataset = dataset.cast_column('audio', Audio())\n",
    "    \n",
    "    # Split the dataset\n",
    "    dataset = dataset.train_test_split(\n",
    "        train_size=train_test_split,\n",
    "        seed=seed\n",
    "    )\n",
    "    \n",
    "    # Create DatasetDict\n",
    "    dataset_dict = DatasetDict({\n",
    "        'train': dataset['train'],\n",
    "        'test': dataset['test']\n",
    "    })\n",
    "    \n",
    "    return dataset_dict\n",
    "\n",
    "# Usage\n",
    "csv_path = \"/home/ruchirverma/Dataset_Folder/csv/whisper_train_dataset.csv\"\n",
    "dataset_dict = create_dataset_from_csv(csv_path)\n",
    "\n",
    "# Verify the dataset\n",
    "print(\"Dataset structure:\")\n",
    "print(dataset_dict)\n",
    "\n",
    "# Print information about each split\n",
    "print(\"\\nTrain split features:\")\n",
    "print(dataset_dict['train'].features)\n",
    "print(f\"Number of training examples: {len(dataset_dict['train'])}\")\n",
    "\n",
    "print(\"\\nTest split features:\")\n",
    "print(dataset_dict['test'].features)\n",
    "print(f\"Number of test examples: {len(dataset_dict['test'])}\")\n",
    "\n",
    "# Print a sample entry\n",
    "print(\"\\nSample entry from training set:\")\n",
    "sample = dataset_dict['train'][0]\n",
    "print(\"Audio info:\")\n",
    "print(f\"Path: {sample['audio']['path']}\")\n",
    "print(f\"Sampling rate: {sample['audio']['sampling_rate']}\")\n",
    "print(f\"Array shape: {sample['audio']['array'].shape}\")\n",
    "print(\"\\nSentence:\")\n",
    "print(sample['sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperFeatureExtractor\n",
    "\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"/home/ruchirverma/whisper_tests/whisper-collabora-finetuned/checkpoint-6000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperTokenizer\n",
    "\n",
    "tokenizer = WhisperTokenizer.from_pretrained(\"/home/ruchirverma/whisper_tests/whisper-collabora-finetuned/checkpoint-6000\", language=\"Hindi\", task=\"transcribe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_long_sequences(batch):\n",
    "    # Encode the text to get token length\n",
    "    tokenized = tokenizer(batch[\"sentence\"])\n",
    "    # Return True if sequence length is within limit\n",
    "    return len(tokenized.input_ids) <= 448\n",
    "\n",
    "# Filter the datasets\n",
    "dataset_dict[\"train\"] = dataset_dict[\"train\"].filter(filter_long_sequences)\n",
    "dataset_dict[\"test\"] = dataset_dict[\"test\"].filter(filter_long_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperProcessor\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(\"/home/ruchirverma/whisper_tests/whisper-collabora-finetuned/checkpoint-6000\", language=\"Hindi\", task=\"transcribe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset_dict[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Audio\n",
    "\n",
    "dataset_dict = dataset_dict.cast_column(\"audio\", Audio(sampling_rate=16000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset_dict[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(batch):\n",
    "    # load and resample audio data from 48 to 16kHz\n",
    "    audio = batch[\"audio\"]\n",
    "\n",
    "    # compute log-Mel input features from input audio array\n",
    "    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
    "\n",
    "    # encode target text to label ids\n",
    "    batch[\"labels\"] = tokenizer(batch[\"sentence\"]).input_ids\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict = dataset_dict.map(prepare_dataset, remove_columns=dataset_dict.column_names[\"train\"], num_proc=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperForConditionalGeneration\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"/home/ruchirverma/whisper_tests/whisper-collabora-finetuned/checkpoint-6000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the columns in the dataset\n",
    "print(dataset_dict[\"train\"].column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.generation_config.language = \"hindi\"\n",
    "model.generation_config.task = \"transcribe\"\n",
    "\n",
    "model.generation_config.forced_decoder_ids = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "    decoder_start_token_id: int\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
    "        # first treat the audio inputs by simply returning torch tensors\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # get the tokenized label sequences\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        # pad the labels to max length\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # if bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "        if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(\n",
    "    processor=processor,\n",
    "    decoder_start_token_id=model.config.decoder_start_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"wer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "\n",
    "    # Replace -100 with pad_token_id in labels for WER calculation\n",
    "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
    "\n",
    "    # Decode both predictions and labels\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    # Calculate Word Error Rate (WER)\n",
    "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
    "    return {\"wer\": wer}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./whisper-collabora_tiny_234hrs\",  # change to a repo name of your choice\n",
    "    per_device_train_batch_size=16,\n",
    "    gradient_accumulation_steps=1,  # increase by 2x for every 2x decrease in batch size\n",
    "    learning_rate=1e-5,\n",
    "    warmup_steps=500,\n",
    "    max_steps=16000,\n",
    "    gradient_checkpointing=True,\n",
    "    fp16=True,\n",
    "    eval_strategy=\"steps\",\n",
    "    per_device_eval_batch_size=16,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=225,\n",
    "    save_steps=2000,\n",
    "    eval_steps=1000,\n",
    "    logging_steps=25,\n",
    "    report_to=[\"tensorboard\"],\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False,\n",
    "    push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=dataset_dict[\"train\"],\n",
    "    eval_dataset=dataset_dict[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=processor.tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.save_pretrained(training_args.output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\n",
    "    \"dataset_tags\": \"allen\",\n",
    "    \"dataset\": \"allen-tts\",  # a 'pretty' name for the training dataset\n",
    "    \"dataset_args\": \"config: hi, split: test\",\n",
    "    \"language\": \"hi\",\n",
    "    \"model_name\": \"Whisper large v3 Turbo Hi - Ruchir Verma\",  # a 'pretty' name for our model\n",
    "    \"finetuned_from\": \"openai/whisper-large-v3-turbo\",\n",
    "    \"tasks\": \"automatic-speech-recognition\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperForConditionalGeneration, WhisperProcessor\n",
    "import torch\n",
    "from jiwer import wer,cer\n",
    "import time\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if device == \"cuda\" else torch.float32\n",
    "\n",
    "# Load the original base Whisper model and processor\n",
    "model_name = \"openai/whisper-large-v3-turbo\"  # or another base Whisper model\n",
    "processor = WhisperProcessor.from_pretrained(model_name)\n",
    "\n",
    "# Now save the tokenizer configuration in the checkpoint directory\n",
    "processor.save_pretrained('/home/ruchirverma/test_models/artpark_whisper/whisper-large-turbo-hi/checkpoint-4000')\n",
    "\n",
    "# After saving, you should be able to load it with the processor\n",
    "model = WhisperForConditionalGeneration.from_pretrained('/home/ruchirverma/test_models/artpark_whisper/whisper-large-turbo-hi/checkpoint-4000').to(device)\n",
    "\n",
    "# Perform inference (for example, transcribing an audio file)\n",
    "def transcribe_audio(audio_path):\n",
    "    import librosa\n",
    "    start_time=time.time()\n",
    "    audio, sampling_rate = librosa.load(audio_path, sr=16000)\n",
    "    input_features = processor(audio, return_tensors=\"pt\", sampling_rate=sampling_rate).input_features\n",
    "    input_features = input_features.to(device)\n",
    "    # Generate transcription\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(input_features)\n",
    "\n",
    "    transcription = processor.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    fc=time.time()-start_time\n",
    "    return transcription, fc\n",
    "\n",
    "# Example usage\n",
    "audio_path = '/home/anirbanmajumder/ds-prototypes/voice/data/recorded_audio_allen_molecule_inheritance/L12-Mole-Bas-Of-Inher-15092021086.wav'  # Replace with your audio file path\n",
    "start=time.time()\n",
    "transcription,fc = transcribe_audio(audio_path)\n",
    "ground_truth=\"दोनों के बीच में आपको क्या ध्यान रखना है, इनमें डिफ़रेंसेज़ क्या है? डीएनए लेवल पर, प्रोकैरियोटिक लेवल पर किसने काम किया था? मेज़ेल्सन और स्टाल ने। क्रोमोसोम लेवल पर।\"\n",
    "werror=wer(ground_truth,transcription)\n",
    "print(f\"WER: {werror}\")\n",
    "print(f\"Transcription: {transcription}\")\n",
    "print(f\"Latency: {time.time()-start} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Function to remove Hindi punctuation\n",
    "def remove_hindi_punctuation(input_string):\n",
    "    # Define the regex pattern for Hindi punctuation\n",
    "    hindi_punctuation_pattern = r\"[।,!?;:”“'()–-]\"  # This pattern matches common punctuation marks\n",
    "    \n",
    "    # Use re.sub to replace punctuation with an empty string\n",
    "    cleaned_string = re.sub(hindi_punctuation_pattern, '', input_string)\n",
    "    \n",
    "    return cleaned_string\n",
    "\n",
    "# Example usage\n",
    "input_text = ground_truth\n",
    "cleaned_text = remove_hindi_punctuation(input_text)\n",
    "print(\"Original Text:\", input_text)\n",
    "print(\"Cleaned Text:\", cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import time\n",
    "import librosa\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "from transformers import pipeline\n",
    "from pydub import AudioSegment\n",
    "from pydub.silence import split_on_silence\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "\n",
    "# Function to calculate WER (Word Error Rate)\n",
    "def calculate_wer(predicted, ground_truth):\n",
    "    return wer(ground_truth, predicted)\n",
    "\n",
    "# Function to calculate CER (Character Error Rate)\n",
    "def calculate_cer(predicted, ground_truth):\n",
    "    return cer(ground_truth, predicted)\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv('/home/ruchirverma/database_scripts/test_set_hindi_stt.csv')\n",
    "\n",
    "# Set the number of random samples you want\n",
    "n_samples = 100  # Change this to your desired number of samples\n",
    "\n",
    "# Randomly sample n rows from the DataFrame\n",
    "# If you want reproducible results, set a seed: random.seed(42)\n",
    "if n_samples >= len(df):\n",
    "    df = df  # Use all rows if n_samples is larger than the dataset\n",
    "    print(f\"Using all {len(df)} rows as the sample size exceeds the dataset size\")\n",
    "else:\n",
    "    df = df.sample(n=n_samples, random_state=42)  # random_state for reproducibility\n",
    "    print(f\"Randomly sampled {n_samples} rows out of {len(df)} total rows\")\n",
    "\n",
    "# Display the sampled rows\n",
    "print(\"\\nSampled rows:\")\n",
    "print(df[['audio_path']].head())\n",
    "\n",
    "\n",
    "# Initialize lists to store the metrics\n",
    "v3_first_chunk_times = []\n",
    "v3_wer_list = []\n",
    "v3_cer_list = []\n",
    "\n",
    "# Loop through each row in the DataFrame to run inference and calculate metrics\n",
    "for index, row in df.iterrows():\n",
    "    audio_path = row['audio_path']\n",
    "    ground_truth_transcript = row['transcript']\n",
    "    print(f\"Transcribing {audio_path}\")\n",
    "    start_time = time.time()\n",
    "    # Start the transcription process\n",
    "    \n",
    "    complete,fc=transcribe_audio(audio_path)\n",
    "    # Run inference for the current audio file\n",
    "    \n",
    "    #ground_truth_transcript,fc=remove_hindi_punctuation(ground_truth_transcript)\n",
    "    # Calculate WER and CER for the current row\n",
    "    current_wer = calculate_wer(complete, ground_truth_transcript)\n",
    "    current_cer = calculate_cer(complete, ground_truth_transcript)\n",
    "    print(f\"WER: {current_wer}\")\n",
    "    print(f\"CER: {current_cer}\")\n",
    "    print(f\"Latency: {fc}\")\n",
    "    \n",
    "    \n",
    "    # Append the metrics to the respective lists\n",
    "    v3_first_chunk_times.append(fc)\n",
    "    v3_wer_list.append(current_wer)\n",
    "    v3_cer_list.append(current_cer)\n",
    "\n",
    "# Calculate average first chunk time\n",
    "v3_average_first_chunk_time = np.mean(v3_first_chunk_times)\n",
    "\n",
    "# Calculate overall WER and CER\n",
    "v3_overall_wer = np.mean(v3_wer_list)\n",
    "v3_overall_cer = np.mean(v3_cer_list)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Average First Chunk Time: {v3_average_first_chunk_time:.2f} seconds\")\n",
    "print(f\"Overall WER: {v3_overall_wer * 100:.2f}%\")\n",
    "print(f\"Overall CER: {v3_overall_cer * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerate launch run_pseudo_labelling.py \\\n",
    "  --model_name_or_path \"/home/ruchirverma/whisper_tests/whisper-medium-ft_full/checkpoint-8000\" \\\n",
    "  --dataset_name \"/home/ruchirverma/dstil_whisper/dataset\" \\\n",
    "  --dataset_config_name \"default\" \\\n",
    "  --dataset_split_name \"train+test\" \\\n",
    "  --text_column_name \"sentence\" \\\n",
    "  --id_column_name \"path\" \\\n",
    "  --output_dir \"./allen_voice_dataset\" \\\n",
    "  --wandb_project \"distil-whisper-labelling\" \\\n",
    "  --per_device_eval_batch_size 64 \\\n",
    "  --dtype \"bfloat16\" \\\n",
    "  --attn_implementation \"sdpa\" \\\n",
    "  --logging_steps 500 \\\n",
    "  --max_label_length 256 \\\n",
    "  --concatenate_audio \\\n",
    "  --preprocessing_batch_size 500 \\\n",
    "  --preprocessing_num_workers 8 \\\n",
    "  --dataloader_num_workers 8 \\\n",
    "  --report_to \"wandb\" \\\n",
    "  --language \"hi\" \\\n",
    "  --task \"transcribe\" \\\n",
    "  --return_timestamps \\\n",
    "  --streaming False \\\n",
    "  --generation_num_beams 1 \\\n",
    "  --push_to_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python create_student_model.py \\\n",
    "  --teacher_checkpoint \"/home/ruchirverma/whisper_tests/whisper-medium-ft_full/checkpoint-8000\" \\\n",
    "  --encoder_layers 24 \\\n",
    "  --decoder_layers 12 \\\n",
    "  --save_dir \"./distil-large-medium-allen-init\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerate launch run_distillation.py \\\n",
    "  --model_name_or_path \"./distil-large-medium-allen-init\" \\\n",
    "  --teacher_model_name_or_path \"/home/ruchirverma/whisper_tests/whisper-medium-ft_full/checkpoint-8000\" \\\n",
    "  --train_dataset_name \"/home/ruchirverma/dstil_whisper/distil-whisper/training/allen_voice_dataset\" \\\n",
    "  --train_split_name \"train\" \\\n",
    "  --text_column_name \"sentence\" \\\n",
    "  --train_dataset_samples \"10000\" \\\n",
    "  --eval_dataset_name \"/home/ruchirverma/dstil_whisper/distil-whisper/training/allen_voice_dataset\" \\\n",
    "  --eval_split_name \"test\" \\\n",
    "  --eval_text_column_name \"sentence\" \\\n",
    "  --eval_steps 1000 \\\n",
    "  --save_steps 500 \\\n",
    "  --warmup_steps 50 \\\n",
    "  --learning_rate 0.0001 \\\n",
    "  --lr_scheduler_type \"constant_with_warmup\" \\\n",
    "  --timestamp_probability 0.2 \\\n",
    "  --condition_on_prev_probability 0.2 \\\n",
    "  --language \"hi\" \\\n",
    "  --task \"transcribe\" \\\n",
    "  --logging_steps 25 \\\n",
    "  --save_total_limit 1 \\\n",
    "  --max_steps 20000 \\\n",
    "  --wer_threshold 20 \\\n",
    "  --per_device_train_batch_size 32 \\\n",
    "  --per_device_eval_batch_size 32 \\\n",
    "  --dataloader_num_workers 8 \\\n",
    "  --preprocessing_num_workers 8 \\\n",
    "  --ddp_timeout 7200 \\\n",
    "  --dtype \"bfloat16\" \\\n",
    "  --attn_implementation \"sdpa\" \\\n",
    "  --output_dir \"./allen-whisper-medium\" \\\n",
    "  --do_train \\\n",
    "  --do_eval \\\n",
    "  --gradient_checkpointing \\\n",
    "  --overwrite_output_dir \\\n",
    "  --predict_with_generate \\\n",
    "  --freeze_encoder \\\n",
    "  --freeze_embed_positions \\\n",
    "  --streaming False \\\n",
    "  --push_to_hub\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "24*16 and 24*12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!df -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict, Audio\n",
    "import numpy as np\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import multiprocessing\n",
    "from functools import partial\n",
    "import threading\n",
    "from queue import Queue\n",
    "multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
